# Introducción {#introduccion}

```{r}
#| include: false
library(tidyverse)
library(kableExtra)
library(DiagrammeR)
library(rethinking)
ggplot2::theme_set(ggplot2::theme_light())
```

Este es un curso de modelación bayesiana aplicada, que se concentra en 
plantear y resolver problemas aplicados usando estadística. Para hacer esto
necesitamos entender tres componentes:

1. El contexto del problema, o conocimiento del área: esto es 
conocimiento científico o de negocios y cómo es que observamos
o medimos el fenómeno que nos interesa.

2. Modelos estadísticos o algoritmos con el que procesamos datos
para obtener resúmenes útiles o informativos acerca de ellos.

3. En vista del contexto del problema (1), ¿cómo construir modelos
estadísticos para responder preguntas de interés?

Este proceso aplica tanto a estadística bayesiana como frecuentista. 


## Diagramas causales

En primer lugar, observamos (@rethinking):

::: callout-note

Las razones de cómo hacemos análisis estadístico (que procedimiento o algoritmo
seleccionamos, por ejemplo) en un problema dado 
**no están en los datos observados**, las **causas** de los datos. 

:::

Las **causas** de los datos no pueden extrarse de los datos solamente. Muchas
veces nos referimos a las causas de los datos como el *proceso generador de los datos*:
esto incluye aspectos del fenómeno que nos interesa (ciencia o proceso de negocios, etc.),
así como el proceso de observación (muestras, valores no observados, etc.).

Consideremos un ejemplo simple para ilustrar este primer principio:

### Ejemplo (cálculos renales) {-}

Este es un estudio real acerca de tratamientos para cálculos renales (@kidney94). Pacientes se asignaron de una forma no controlada a dos tipos de tratamientos para reducir cálculos renales. Para cada paciente, conocemos el tipo de ćalculos que tenía (grandes o chicos) y si el tratamiento tuvo éxito o no.

La tabla original tiene 700 renglones (cada renglón es un paciente)

```{r, message = FALSE}
calculos <- read_csv("../datos/kidney_stone_data.csv")
names(calculos) <- c("tratamiento", "tamaño", "éxito")
calculos <- calculos |> 
   mutate(tamaño = ifelse(tamaño == "large", "grandes", "chicos")) |> 
   mutate(resultado = ifelse(éxito == 1, "mejora", "sin_mejora")) |> 
   select(tratamiento, tamaño, resultado)
nrow(calculos)
```

y se ve como sigue (muestreamos algunos renglones):

```{r, message = FALSE}
calculos |> 
   sample_n(10) |> kable() |> 
   kable_paper(full_width = FALSE)
```

Aunque estos datos contienen información de 700 pacientes, los datos pueden resumirse sin pérdida de información contando como sigue:

```{r}
calculos_agregada <- calculos |> 
   group_by(tratamiento, tamaño, resultado) |> 
   count()
calculos_agregada |> kable() |> 
   kable_paper(full_width = FALSE)
```

Como en este caso nos interesa principalmente la tasa de éxito de cada tratamiento, podemos mejorar mostrando como sigue:

```{r}
calculos_agregada |> pivot_wider(names_from = resultado, values_from = n) |> 
   mutate(total = mejora + sin_mejora) |> 
   mutate(prop_mejora = round(mejora / total, 2)) |> 
   select(tratamiento, tamaño, total, prop_mejora) |> 
   arrange(tamaño) |> 
   kable() |> 
   kable_paper(full_width = FALSE)
```

Esta tabla descriptiva es una reescritura de los datos, y no hemos resumido nada todavía. Pero es apropiada para empezar a contestar la pregunta:

-   ¿Qué indican estos datos acerca de qué tratamiento es mejor? ¿Acerca del tamaño de cálculos grandes o chicos?

Supongamos que otro analista decide comparar los pacientes que recibieron cada tratamiento, ignorando la variable de tamaño:

```{r}
calculos |> group_by(tratamiento) |> 
   summarise(prop_mejora = mean(resultado == "mejora") |> round(2)) |> 
   kable() |> 
   kable_paper(full_width = FALSE)
```

y parece ser que el tratamiento $B$ es mejor que el $A$. Esta es una paradoja (un ejemplo de la [paradoja de Simpson](https://es.wikipedia.org/wiki/Paradoja_de_Simpson)) . Si un médico no sabe que tipo de cálculos tiene el paciente, ¿entonces debería recetar $B$? ¿Si sabe debería recetar $A$? Esta discusión parece no tener mucho sentido.

Podemos investigar por qué está pasando esto considerando la siguiente tabla, que solo examina cómo se asignó el tratamiento dependiendo del tipo de cálculos de cada paciente:

```{r}
calculos |> group_by(tratamiento, tamaño) |> count() |> 
   kable() |> 
   kable_paper(full_width = FALSE)
```

Nuestra hipótesis aquí es que la decisión de qué tratamiento usar depende del tamaño de los cálculos.  En este caso, hay una decisión pues A es una cirugía y B es un procedimiento
menos invasivo, y se prefiere utilizar el tratamiento $A$ para cálculos grandes, y $B$ para cálculos chicos. Esto quiere decir que en la tabla total *el tratamiento* $A$ está en desventaja porque se usa en casos más difíciles, pero el tratamiento $A$ parece ser en general mejor. La razón es probablemente un proceso de optimización de recursos y riesgo que hacen los doctores.

- En este caso, una mejor respuesta a la pregunta
de qué tratamiento es mejor es la que presenta los datos desagregados.
- La tabla desagregada de asignación del tratamiento nos informa acerca de cómo se está distribuyendo el tratamiento
en los pacientes.

::: callout-note
Los resúmenes descriptivos acompañados de hipótesis 
causales acerca del *proceso generador de datos*, nos guía hacia descripciones 
interpretables de los datos. 
:::

Las explicaciones no son tan simples y, otra vez, interviene el comportamiento de doctores, tratamientos, y distintos tipos de padecimientos.

Podemos codificar la información causal con un diagrama:

```{r}
#| label: kidney_stone_dag
#| code-fold: true
grViz("
digraph {
  graph [ranksep = 0.2]
  node [shape=plaintext]
    T 
    M 
    C
  edge [minlen = 3]
    T -> M
    C -> T
    C -> M
{ rank = same; M; T }
}
", width = 200, height = 50)

```

Es decir, el tamaño de los cálculos es una causa común de tratamiento (T)
y resultado (M). Veremos más adelante que la decisión 
de condicionar a el tipo de cálculos proviene
de un análisis relativamente simple de este diagrama causal, independientemente
de los métodos que usemos para estimar las proporciones de interés (en este
ejemplo, examinar las tablas cruzadas es equivalente a hacer estimaciones
de máxima verosimlitud).

---

El ejemplo de arriba es un DAG (Gráfica dirigida acíclica), y no son 
generados por datos observados, sino que codifican conocimiento acerca
del fenómenos y los datos observados. Nos ayudan a (@rethinking):

- Pensar claramente en términos científicos/de negocio acerca de nuestro problema
- Expresar los supuestos que hacemos que soportan nuestro análisis
- Entender qué podemos entender o explicar, sin hacer supuestos adicionales acerca
de las relaciones particulares entre las variables.
- Guiar el análisis para decidir que modelos o procedimientos usar para contestar preguntas de interés.


Los DAGs se construyen con causas, no asociaciones. 
El pensamiento causal es útil siempre que queremos responder preguntas
acerca de un fenómeno de interés:

Análisis descriptivo

1. Como vimos en el ejemplo anterior, incluso el análisis descriptivo (qué tabla
usar, qué gráfica usar)
   de datos requiere de un análisis causal.
2. Muchas veces los datos que tenemos, por distintas razones, tienen características
que requieren procesarlos (por ejemplo ponderarlos) para que nos den respuestas
entendibles.

Inferencia causal:

1. En algunos casos, queremos saber consecuencias de una intervención sobre
un sistema o proceso dados (por ejemplo, ¿cuántos accidentes graves habría
si pusiéramos una multa por no usar cinturón de seguridad?). Esto requiere utilizar pensamiento causal.
2. También es usual necesitar pensar cómo serían las cosas si el pasado se hubiera
desarrollado de manera distinta (por ejemplo, ¿cómo serían las ventas si no se hubiera gastado en publicidad?)
en publicidad ?).

Diseño de estudios o experimentos. 

Si queremos recolectar datos acerca
de un fenómeno particular (por ejemplo, ¿cómo debo seleccionar una muestra para
medir orientación política de una población?), diseños eficientes requieren tener
conocimiento de dominio acerca de las causas de las variables que nos interesa medir.
Por ejemplo, si queremos tomar una muestra de casillas para estimar el resultado
de una votación, deberíamos considerar variables geográficas como distrito electoral,
grado de urbanización, etc.


## Modelos y procedimientos

En muchos cursos introductorios de estadística se muestran distintos
tipos de procedimientos, que aplican según el tipo de datos (por ejemplo,
categóricos o numéricos, pareados, no pareados, etc), generalmente con el
propósito de evaluar evidencia en contra de una hipótesis nula. 

Este enfoque puede ser confuso en un principio (¿cómo se relacionan todos
estos procedimientos?), y también restringir nuestra capacidad para analizar
datos: ¿qué hacemos cuando no se cumplen los supuestos de un procedimiento?
Adicionalmente, la manera en que fallan estas herramientas puede ser poco intuitiva
y difícil de descubrir.

Adiconalmente, aunque son herramientas poderosas, no sustituyen el pensamiento científico
o de proceso de negocios. Estas herramientas no generan hallazgos si no 
están acompañados de pensamiento causal.


Este curso tiene dos propósitos: 

1. Dar herramientas (bayesianas) para analizar datos que son más flexibles, y 
se puedan adaptar a distintas situaciones.
2. Proponer un proceso para analizar datos, que sea más sistemático, robusto,
y maneras de checar que el proceso es correcto o hace lo que pensamos que tiene
qué hacer.
3. Entender cómo se conectan estos modelos o métodos de analizar de datos
con preguntas reales acerca del fenómeno de interés.

## Proceso de modelación

El proceso de modelación que propondremos es bayesiano, y propondremos varios
pasos para analizar datos:

- Análisis como *software*: Una parte de este proceso está relacionado con la reproducibilidad y documentación
del trabajo, y su objetivo es evitar errores de programación y de organización
(esta parte hablaremos menos: es necesario seguir los estándares de la industria para
obtener resultados más confiables).

- Otra parte es el proceso con el cual construimos y contrastamos 
modelos para contestar preguntas, verificamos los modelos y sus respuestas y 
checamos resultados de cómputos.


## Análisis como software


## Análisis como proceso

Iremos refinando nuestro poco a poco, conforme veamos distintas herramientas y problemas.
El más básico es el siguiente (@rethinking):

1. Definir un modelo generativo para la muestra de datos.
2. Definir la cantidad que queremos estimar en relación al fenómeno de interés.
3. Definir un proceso estadístico para hacer una estimación.
4. Probar el proceso 3) usando 1).
5. Analizar los datos, resumir resultados.

Este proceso no es exclusivo de los modelos bayesianos, pero quizá es más natural,
como veremos, cuando adoptamos el punto de vista bayesiano.















